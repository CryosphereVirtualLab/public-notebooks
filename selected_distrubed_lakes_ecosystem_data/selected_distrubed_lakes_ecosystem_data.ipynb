{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff74bff0",
   "metadata": {},
   "source": [
    "# \"Ecosystem data for selected disturbed lakes in permafrost regions\" \n",
    "Cryosphere Virtual Lab, Clemens von Baeckmann, Copyright (c) 2022/2023.\n",
    "\n",
    "This notebook generates a georeferenced dataset for disturbed areas with focus on lake area change in permafrost regions. The generated data aims to advance the understanding of a changing cryosphere and the corresponding change of ecosystems derived with the edited landcover dataset. This project combine and edit multi-source data sets, implementing the novel CVL environment.\n",
    "\n",
    "The landcover dataset from Bartsch et al. (2019) available at the CVL Data search, serves as foundation, focusing on selected lakes. Auxiliary data will be added and in the final step the data will be prepared for the use in the CVL3D Viewer of the cryosphere virtual laboratory webpage.\n",
    "\n",
    "\n",
    "<em>Lets start!</em>\n",
    "\n",
    "________________________\n",
    "\n",
    "### Load the data:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27766ec",
   "metadata": {},
   "source": [
    "- first import the python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd7b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import zipfile\n",
    "import pygrib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio import mask as msk \n",
    "#from matplotlib import pyplot as plt  \n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal, ogr\n",
    "import geopandas as gpd\n",
    "from IPython.display import Image\n",
    "import requests\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265f3a1",
   "metadata": {},
   "source": [
    "- now <b>define</b> if data needs to be downloaded or is already stored locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6ecf34-09ac-4b1b-a446-699ede734e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "download_data = True #insert False or True if you need to download the data\n",
    "need_to_download_era5 = True # do you want additional era5data? and need to download era5  data? False or True\n",
    "\n",
    "buffer_dist = 30 #buffer dist (CVL proposal 30 meter)\n",
    "buffer_dist_era5 = 10000 #define era5 buffer dist around the lake, check era5 data grid to have input data!)\n",
    "\n",
    "#trend data: (...found via the CVL Metadata Search)\n",
    "Lakes_url = 'https://hs.pangaea.de/sat/NitzeI-etal_2018/Lakes_T1.zip'\n",
    "#landcover data: (...additional data)\n",
    "Landcover_url = 'https://hs.pangaea.de/sat/GlobPermafrost_Sentinel/Landcover/ZAM_LCP_LANDC_SEN12_V02_20150815_20180830_T01.tif'\n",
    "\n",
    "if os.path.exists('data') == False:\n",
    "    os.makedirs('data')\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b2f15",
   "metadata": {},
   "source": [
    "- load the available data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf786a37-52ff-4c19-8340-60b3ac8ebf80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please wait...\n",
      "ds_landcover loaded\n",
      "ds_trend loaded\n",
      "ALL main datasets are loaded!\n"
     ]
    }
   ],
   "source": [
    "# download & read data\n",
    "print('please wait...')\n",
    "\n",
    "if download_data == True:\n",
    "    print('___loading Landcover')\n",
    "    \n",
    "    if os.path.exists('data/ds_landcover.tif') == False:\n",
    "        #download_string = 'wget -O data/ds_landcover.tif {}'.format(Landcover_url)\n",
    "        #os.system(download_string)\n",
    "        r = requests.get(Landcover_url, allow_redirects=True)\n",
    "        open('data/ds_landcover.tif', 'wb').write(r.content)\n",
    "        \n",
    "    ds_landcover =  rio.open(Landcover_url)\n",
    "\n",
    "    print('___loading Trends')\n",
    "    if os.path.exists('data/Lakes_T1/T1_WS_Lakes.shp') == False:\n",
    "        print ('downloading and open dataset trend-Lakes')\n",
    "        #download_string = 'wget -O data/Lakes_T1.zip {}'.format(Lakes_url) \n",
    "        #os.system(download_string)\n",
    "        r = requests.get(Lakes_url, allow_redirects=True)\n",
    "        open('data/Lakes_T1.zip', 'wb').write(r.content)\n",
    "        with zipfile.ZipFile('data/Lakes_T1.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "        print('data downloaded')\n",
    "\n",
    "    ds_trend = gpd.read_file('data/Lakes_T1/T1_WS_Lakes.shp')          \n",
    "    print('dataset loaded')\n",
    "    \n",
    "else:\n",
    "    ds_landcover = rio.open('data/ds_landcover.tif')\n",
    "    print('ds_landcover loaded')\n",
    "    ds_trend = gpd.read_file('data/Lakes_T1/T1_WS_Lakes.shp')   \n",
    "    print('ds_trend loaded')\n",
    "\n",
    "print('ALL main datasets are loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1cf2dc",
   "metadata": {},
   "source": [
    "- to download the additional era5 data: <br>\n",
    "    please <b>register</b> and <b>install</b> the `cdsapi` key by following this link: https://cds.climate.copernicus.eu/api-how-to <br>\n",
    "    You will also need to accept the _Licence to use Copernicus Products_ once at https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products  <br>\n",
    "    use this code below or download it manually: reanalysis-era5-single-levels: precipitation and temperature<br>\n",
    "    https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=form<br>\n",
    "    \n",
    "    <em>info: the data needs to be stored in: /data/era5    </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe03c539-90c1-4a4f-90ad-0e4cc67ccc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "if need_to_download_era5 == True:\n",
    "    import cdsapi\n",
    "    \n",
    "    if not os.path.exists(os.path.join('data','era5')):\n",
    "        os.makedirs(os.path.join('data','era5'))\n",
    "    \n",
    "    print('download the total_precipitation data')\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'format': 'grib',\n",
    "            'variable': 'total_precipitation',\n",
    "            'year': [\n",
    "                '2018', '2019', '2020',\n",
    "                '2021',\n",
    "            ],\n",
    "            'month': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "            ],\n",
    "            'day': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "            ],\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00',\n",
    "                '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00',\n",
    "                '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00',\n",
    "                '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00',\n",
    "                '21:00', '22:00', '23:00',\n",
    "            ],\n",
    "            'area': [\n",
    "                74, 65, 65,\n",
    "                74,\n",
    "            ],\n",
    "        },\n",
    "        'data/era5/yamal_precip.grib')\n",
    "\n",
    "\n",
    "    #and 2m_temperature data\n",
    "\n",
    "    print('download the 2m_temperature data')\n",
    "    #c = cdsapi.Client()\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'format': 'grib',\n",
    "            'variable': '2m_temperature',\n",
    "            'year': [\n",
    "                '2018', '2019', '2020',\n",
    "                '2021',\n",
    "            ],\n",
    "            'month': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "            ],\n",
    "            'day': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "            ],\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00',\n",
    "                '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00',\n",
    "                '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00',\n",
    "                '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00',\n",
    "                '21:00', '22:00', '23:00',\n",
    "            ],\n",
    "            'area': [\n",
    "                74, 65, 65,\n",
    "                74,\n",
    "            ],\n",
    "        },\n",
    "        'data/era5/yamal_temp.grib')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97528ef",
   "metadata": {},
   "source": [
    "- dataset about geocryological conditions:<br><br>\n",
    "    ...the dataset found via the CVL Metadata Search was is not accessible* and therefore is not included: \n",
    " \n",
    "    <em>Vladimir, Slavin-Borovsky; E.S., Melnikov (2004): Regionalization for geocryological conditions and forecast, Yamal Peninsula, Russia. NSIDC: National Snow and Ice Data Center, Boulder, Colorado USA, https://nsidc.org/data/ggd193/versions/1 </em>\n",
    "\n",
    "    <em>*National Snow and Ice Data Center is informed</em><br>\n",
    "<br>\n",
    "- Sentinel-2 data:\n",
    "    found on the CVL Metadata Search: <br><br>\n",
    "                'S2A_MSIL1C_20170801T071621_N0205_R006_T42WVD_20170801T071618'<br>\n",
    "                'S2A_MSIL1C_20170707T080611_N0205_R078_T42XWF_20170707T080606'<br> <br>\n",
    "    ...but data from the tile requested is only available for 1 month, and not in the long term archive.<br><br>\n",
    "    open issue about that: (last accessed 30.01.2023)<br>\n",
    "    https://github.com/CryosphereVirtualLab/public-notebooks/issues/18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f3835",
   "metadata": {},
   "source": [
    "------\n",
    "### process the data:\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa3cf7",
   "metadata": {},
   "source": [
    "- from the trend dataset we select only lakes of interest:\n",
    "    <b>define</b> the \"nt_ch_pc\" threshold here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576ab2f8-9ad7-4daa-b9c3-282514f2b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt_ch_pc is set to: -99\n",
      "115 lakes are loaded\n"
     ]
    }
   ],
   "source": [
    "nt_ch_pc_value = -99 #-99 includes 115 lakes\n",
    "Lakes_OI = copy.deepcopy(ds_trend[ds_trend.nt_ch_pc <= nt_ch_pc_value]).reset_index(drop=True)\n",
    "print('nt_ch_pc is set to: {}'.format(nt_ch_pc_value))\n",
    "print('{} lakes are loaded'.format(len(Lakes_OI)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35031d5",
   "metadata": {},
   "source": [
    "- the landcover data is masked for the area of the lakes of interest:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a365d4be-67f6-48fe-a8ac-5aa311c99ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut landcover with shape\n",
      "doing 115 lakes\n",
      "errors with 172200\n",
      "errors with 172343\n",
      "errors with 172388\n",
      "errors with 172504\n",
      "errors with 173070\n",
      "errors with 173681\n",
      "errors with 174085\n",
      "errors with 174550\n",
      "errors with 174600\n",
      "errors with 174675\n",
      "errors with 174720\n",
      "errors with 174848\n",
      "errors with 174884\n",
      "errors with 174937\n",
      "errors with 174952\n",
      "problems with 15 lakes\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('cut landcover with shape')\n",
    "print( 'doing {} lakes'.format(len(Lakes_OI.index)))\n",
    "\n",
    "count_rm = 0\n",
    "for i in Lakes_OI.index:\n",
    "    #Lake_ID = Lakes_OI.ID[i]\n",
    "    Lake = copy.deepcopy(Lakes_OI[Lakes_OI.index == i]).reset_index(drop = True)\n",
    "    Lake_ID = Lake.ID\n",
    "\n",
    "    #generate now for each lake an folder with all the calculated values:\n",
    "    data_lake_path = 'data/lake_data/ID{}'.format(np.array(Lake_ID)[0])\n",
    "    if os.path.exists(data_lake_path) == False:\n",
    "        os.makedirs(data_lake_path)\n",
    "        #print(Lake_ID)\n",
    "        #print('masking now the landcover with lake {}'.format(np.array(Lake_ID)[0]))\n",
    "\n",
    "        try: \n",
    "            Lake = Lake.to_crs(crs = 32642)\n",
    "            Lake.geometry = copy.deepcopy(Lake.buffer(buffer_dist,join_style=1))\n",
    "\n",
    "            out_image, out_transform = msk.mask(ds_landcover, Lake.geometry, crop=True)\n",
    "            out_meta = ds_landcover.meta\n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": out_image.shape[1],\n",
    "                             \"width\": out_image.shape[2],\n",
    "                             \"transform\": out_transform})\n",
    "\n",
    "            write_tif = '{}/ID{}.tif'.format(data_lake_path,np.array(Lake_ID)[0])\n",
    "\n",
    "            with rio.open(write_tif, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "                \n",
    "        except:\n",
    "            print('errors with', np.array(Lake_ID)[0])\n",
    "            count_rm = count_rm + 1\n",
    "\n",
    "print('problems with', count_rm, 'lakes')\n",
    "print('done')\n",
    "\n",
    "#to remove all generated files:\n",
    "#rm 'data/lake_data' -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05cc23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the error data\n",
    "for folder in os.listdir('data/lake_data'):\n",
    "    search = 'data/lake_data/{}'.format(folder)\n",
    "    if not os.listdir(search):\n",
    "        os.rmdir(search)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c715a8",
   "metadata": {},
   "source": [
    "- if there is era5 data, the data is cutted for each of the lakes. <br>\n",
    "<em>This can take very long, depending on the used \"nt_ch_pc\" threshold, and your CPU/RAM speed</em> <br><br>\n",
    "...<b>change</b> the decision to False or True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e62552-d71c-436d-824c-3fa76fcbab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "decision = False\n",
    "\n",
    "if decision:\n",
    "    print('continue... please wait...')\n",
    "    \n",
    "    data = r'data/era5'\n",
    "    data_paths = []\n",
    "    for root, dirs, files in os.walk(data):\n",
    "            for d in files:\n",
    "                if d.endswith('.grib'):\n",
    "                    data_paths.append(root+'/'+d)\n",
    "                    \n",
    "    for i in range(0,len(data_paths)):\n",
    "        print('reading data')\n",
    "        print(data_paths[i])\n",
    "        grbs = pygrib.open(data_paths[i])\n",
    "        \n",
    "        grbs_sel = grbs.select()\n",
    "        \n",
    "        print(f\"processing lakes for {grbs_sel[0].parameterName}\")\n",
    "        \n",
    "        for i in Lakes_OI.index:\n",
    "            print(i+1, '/', len(Lakes_OI))\n",
    "            #Lake_ID = Lakes_OI.ID[i]\n",
    "            Lake = copy.deepcopy(Lakes_OI[Lakes_OI.index == i]).reset_index(drop = True)\n",
    "            Lake = Lake.to_crs(crs = 32642)\n",
    "            Lake_ID = Lake.ID\n",
    "    \n",
    "            data_lake_path = 'data/lake_data/ID{}'.format(np.array(Lake_ID)[0])\n",
    "\n",
    "            date = []\n",
    "            mean = []\n",
    "            median = []\n",
    "            min_ = []\n",
    "            max_ = []\n",
    "            year = []\n",
    "            month = []\n",
    "            day = []\n",
    "    \n",
    "            if os.path.exists(data_lake_path) == True:\n",
    "                cut_data = copy.deepcopy(Lake)\n",
    "                cut_data.geometry = copy.deepcopy(Lake.buffer(buffer_dist_era5,join_style=1))\n",
    "                begin_of_dataset = 2015\n",
    "    \n",
    "                last_year = 0\n",
    "                last_month = 0\n",
    "                \n",
    "                outfile = '{}/era5_{}_ID{}.pkl'.format(data_lake_path, grbs_sel[0].parameterName,np.array(Lake_ID)[0])\n",
    "            \n",
    "                if not os.path.isfile(outfile):\n",
    "                    print('masking .grib file to extend in the defining shapefile')\n",
    "                    print('using dataset since:',begin_of_dataset)\n",
    "\n",
    "                    for grb in grbs_sel:\n",
    "\n",
    "                        #input(grb.keys()) \n",
    "                        # for i in grb.keys():\n",
    "                        #     try:\n",
    "                        #         output = str(grb[i])\n",
    "                        #     except:\n",
    "                        #         output = '_nan_'\n",
    "                        #     print('\\n{}\\t\\t{}'.format(i,output))\n",
    "\n",
    "                        data = grb.latLonValues\n",
    "                        ts = str(grb.julianDay).replace('.','_') \n",
    "                        write = False\n",
    "                        if grb.year >= begin_of_dataset:\n",
    "                            write = True\n",
    "                            #if not grb.year == last_year:\n",
    "                            #    print('Year:',grb.year)\n",
    "                            #last_year = grb.year\n",
    "\n",
    "                            #if not grb.month == last_month:\n",
    "                            #    print('Month:',grb.month)\n",
    "                            #last_month = grb.month\n",
    "\n",
    "\n",
    "                            lat_lon_val = ([],[],[])\n",
    "                            u = 0\n",
    "                            for j in range(0,(len(data))):\n",
    "                                k = 0\n",
    "                                if u == 2:\n",
    "                                    u = 0\n",
    "                                    k = 1\n",
    "                                    if 'temperature' in grb.parameterName:\n",
    "                                        lat_lon_val[2].append(data[j]-273.15) #unit K to °C\n",
    "                                        scale = 'Grad Celsius'\n",
    "                                    if 'precipitation' in grb.parameterName: \n",
    "                                        lat_lon_val[2].append(data[j]*1000)  #unit m to mm  \n",
    "                                        scale = 'Millimeter'\n",
    "                                if u == 1:\n",
    "                                    u = u + 1\n",
    "                                    lat_lon_val[1].append(data[j])\n",
    "                                if u == 0 and k == 0:\n",
    "                                    u = u + 1\n",
    "                                    lat_lon_val[0].append(data[j])\n",
    "\n",
    "                            lat_lon_val =np.array(lat_lon_val)\n",
    "                            lat_lon_val=lat_lon_val.transpose()\n",
    "\n",
    "                            pointDf = pd.DataFrame(lat_lon_val)\n",
    "                            pointDf.rename( columns={2 :'values'}, inplace=True )\n",
    "\n",
    "                            gdf = gpd.GeoDataFrame(pointDf['values'],geometry=gpd.points_from_xy(pointDf[1],pointDf[0]),crs=4326)\n",
    "                            #cut_data = gpd.read_file(cut_shape)\n",
    "\n",
    "                            cut_data = copy.deepcopy(cut_data.to_crs(4326))\n",
    "                            gdf = gdf.to_crs(4326) \n",
    "\n",
    "                            #df1 = gpd.read_file(r'D:\\Yamal\\temp_precipitation_plot\\cropPoints1.shp')\n",
    "                            res_intersection = gdf.overlay(cut_data, how='intersection')\n",
    "                            if 'id' in res_intersection.columns:\n",
    "                                del res_intersection['id']\n",
    "\n",
    "                            #want to check the correct cutted extend:\n",
    "                            #res_intersection.to_file(r'D:\\Yamal\\data2\\Landcover_self_generated\\ID45136\\test.shp', driver='ESRI Shapefile')\n",
    "\n",
    "                            date.append(grb.julianDay)\n",
    "                            mean.append(np.mean(res_intersection['values']))\n",
    "                            median.append(np.median(res_intersection['values']))\n",
    "                            min_.append(np.min(res_intersection['values']))\n",
    "                            max_.append(np.max(res_intersection['values']))\n",
    "                            year.append(grb.year)\n",
    "                            month.append(grb.month)\n",
    "                            day.append(grb.day)\n",
    "\n",
    "                    print('generating and writing the masked database', grb.parameterName,' to main folder')\n",
    "                    database = pd.DataFrame({'ID':grb.parameterName,\n",
    "                                             'date':date,\n",
    "                                             'year':year,\n",
    "                                             'month':month,\n",
    "                                             'day':day,\n",
    "                                             'mean':mean,\n",
    "                                             'median':median,\n",
    "                                             'min':min_,\n",
    "                                             'max':max_})\n",
    "\n",
    "                    outfile = '{}/era5_{}_ID{}.csv'.format(data_lake_path, grb.parameterName,np.array(Lake_ID)[0])\n",
    "                    database.to_csv(outfile)  # where to save it\n",
    "                    print(outfile)\n",
    "\n",
    "    print('done with era5 data cutting')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df311b2",
   "metadata": {},
   "source": [
    "---\n",
    "### Produce output data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd29cbd",
   "metadata": {},
   "source": [
    "- now we produce some results with the processed data. Cut the landcover with each lake, calc indices and save the histogram\n",
    "<br><em>evaluation of the existing data... calc indices... calc landcover distribution...</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57258eeb-f84b-4917-b435-4df8e4665c02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please wait...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('please wait...')\n",
    "\n",
    "for i in Lakes_OI.index:\n",
    "    #Lake_ID = Lakes_OI.ID[i]\n",
    "    Lake = copy.deepcopy(Lakes_OI[Lakes_OI.index == i]).reset_index(drop = True)\n",
    "    Lake_ID = Lake.ID\n",
    "\n",
    "    #print(Lake_ID[0])\n",
    "    data_lake_path = 'data/lake_data/ID{}'.format(np.array(Lake_ID)[0])\n",
    "    \n",
    "    if os.path.exists(data_lake_path) == True:\n",
    "        for file in os.listdir(data_lake_path):\n",
    "            if 'ID' and '.tif' in file:\n",
    "                infile = data_lake_path + '/' + file\n",
    "    \n",
    "        #analyze the cutted lake\n",
    "        ds = gdal.Open(infile, gdal.GA_ReadOnly)\n",
    "        b_ds = ds.GetRasterBand(1) \n",
    "        b_ds_data = b_ds.ReadAsArray().astype(np.float32)\n",
    "\n",
    "        #make the nan values\n",
    "        dimension = np.shape(b_ds_data)\n",
    "        db_image = copy.deepcopy(b_ds_data)\n",
    "        for i in range(dimension[0]):\n",
    "            for j in range(dimension[1]):\n",
    "                if b_ds_data[i,j] == 255:\n",
    "                    db_image[i,j]=np.nan\n",
    "\n",
    "        #prepare the data\n",
    "        classes, count = np.unique(db_image,return_counts=True)\n",
    "        tot_sum = np.sum(count[0:(len(count)-1)]) #last value from count is count of nan values which is negligible\n",
    "        real_classes = real_classes = range(1,22)\n",
    "\n",
    "        labels = ['Sparse vegetation',\n",
    "                  'Sparse vegetation',\n",
    "                  'Shrub tundra',\n",
    "                  'Shrub tundra',\n",
    "                  'Shrub tundra',\n",
    "                  'Shrub tundra',\n",
    "                  'Shrub tundra',\n",
    "                  'Forest',\n",
    "                  'Forest',\n",
    "                  'Forest',\n",
    "                  'Grassland',\n",
    "                  'Floodplain',\n",
    "                  'Disturbed',\n",
    "                  'Floodplain',\n",
    "                  'Floodplain',\n",
    "                  'Floodplain',\n",
    "                  'Barren',\n",
    "                  'Barren',\n",
    "                  'Water',\n",
    "                  'Water',\n",
    "                  'Water']\n",
    "        \n",
    "        #__________________________________________________________________________\n",
    "        #calc main classes     #calc main classes     #calc main classes  \n",
    "        \n",
    "        data = []\n",
    "        for i in real_classes:\n",
    "            data.append(i)\n",
    "        data = pd.DataFrame({'classes':data})\n",
    "        labels = pd.DataFrame(labels)\n",
    "        df = data.join(labels)\n",
    "        dataframe = pd.DataFrame({'classes':classes[0:-1], 'count':count[0:-1]})\n",
    "        class_data = df.set_index('classes').join(dataframe.set_index('classes'))\n",
    "        class_data = class_data.reset_index()\n",
    "        three_main_classes = class_data.sort_values(by=['count'],ascending=False)[0:3]\n",
    "        #print('main classes are:',list(three_main_classes[0]))\n",
    "        Lake['main_class'] = str(list(three_main_classes[0]))\n",
    "\n",
    "        #percent\n",
    "        pixel_summe = np.sum(count[0:-1]) #exclude nan count!\n",
    "        prozente = (count[0:-1]/pixel_summe)*100\n",
    "        present_classes = len(classes[0:-1])\n",
    "        #print('present_classes:', present_classes)\n",
    "\n",
    "        #__________________________________________________________________________\n",
    "        #diversity index     diversity index     diversity index     diversity index\n",
    "        \n",
    "        #1 = divers 0 = mono\n",
    "        n_classes = len(dataframe)\n",
    "        # Diversity_in_classes_Index\n",
    "        DIC_I = n_classes / len(real_classes)\n",
    "        temp = class_data.sort_values(by=['count'],ascending=False)\n",
    "        temp = list(temp['count'])\n",
    "        pixel_summe = np.nansum(temp) \n",
    "                \n",
    "        # Verteilung\n",
    "        verteilung = []\n",
    "        for i in temp:\n",
    "            verteilung.append((i / pixel_summe)**2) \n",
    "\n",
    "        Div_index = 1 - np.nansum(verteilung)\n",
    "        Div_index = np.round(Div_index,3)\n",
    "        Lake['div_index'] = Div_index\n",
    "        # print('diversity index:', Div_index)\n",
    "        \n",
    "        #save .shp file\n",
    "        outfile = data_lake_path + '/shape_ID{}.shp'.format(Lake_ID[0])\n",
    "        Lake.to_file(outfile)  \n",
    "        \n",
    "        #__________________________________________________________________________\n",
    "        #histogram     histogram     histogram     histogram     histogram\n",
    "        \n",
    "        outfile = data_lake_path + '/landcover_classes_ID{}.jpg'.format(Lake_ID[0])\n",
    "        plt.title('Landcover classes for Lake: {}'.format(Lake_ID[0]))\n",
    "        plt.bar(classes[0:(len(count)-1)],count[0:(len(count)-1)])\n",
    "        plt.xlabel('Landcover classes')\n",
    "        plt.ylabel('n of classified Pixel')\n",
    "        plt.xticks(real_classes)\n",
    "        \n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        plt.text(0,0.85,'Diversity Index: {}'.format(Div_index),\n",
    "                 fontsize='xx-large',\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom',\n",
    "                 #transform=ax.transAxes,\n",
    "                bbox=props)\n",
    "        \n",
    "        plt.savefig(outfile)\n",
    "        plt.close('all')\n",
    "        #print('saved_something')\n",
    "        #input('press any key to continue')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffd20d",
   "metadata": {},
   "source": [
    "- produce some additional output data <em>(if era5 data was cutted and downloaded)</em>: <br>\n",
    "    ...generate the era5 plots for each lake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89696453-2248-449d-9ce4-20592f10fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please wait...\n",
      "0 lakes have no era5 plots\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('please wait...')\n",
    "\n",
    "df_temp = []\n",
    "df_precip = []\n",
    "zero_values_found = []\n",
    "for i in Lakes_OI.index:\n",
    "    #print(i+1, '/', len(Lakes_OI))\n",
    "    Lake = copy.deepcopy(Lakes_OI[Lakes_OI.index == i]).reset_index(drop = True)\n",
    "    Lake = Lake.to_crs(crs = 32642)\n",
    "    Lake_ID = Lake.ID\n",
    "    data_lake_path = 'data/lake_data/ID{}'.format(np.array(Lake_ID)[0])\n",
    "    path_1 = []\n",
    "    path_2 = []\n",
    "    run = False\n",
    "    if os.path.exists(data_lake_path) == True:\n",
    "        for file in os.listdir(data_lake_path):\n",
    "            if '.csv' in file: \n",
    "                if 'temperature' in file:\n",
    "                    path_1 = data_lake_path + '/' + file\n",
    "                    df_temp = pd.read_csv(path_1)\n",
    "                    run = True\n",
    "                if 'precipitation' in file:\n",
    "                    path_2 = data_lake_path + '/' + file\n",
    "                    df_precip = pd.read_csv(path_2)\n",
    "                    run = True\n",
    "\n",
    "        #df_temp = pd.read_pickle(path_1)\n",
    "        #df_precip = pd.read_pickle(path_2)\n",
    "        \n",
    "        if run == True:\n",
    "            c=0\n",
    "\n",
    "            #__________________________________________________________________________ERA5\n",
    "            ##     temperature      temperature       temperature       temperature\n",
    "\n",
    "            #daten aufbereiten\n",
    "            year_months_mean = []\n",
    "            year_months_min = []\n",
    "            year_months_max = []\n",
    "\n",
    "            for i in np.unique(df_temp.month):\n",
    "                temp_mean=[]\n",
    "                temp_min=[]\n",
    "                temp_max=[]\n",
    "                for j in np.unique(df_temp.year):\n",
    "                    temp_mean.append(np.mean(df_temp['mean'][(df_temp.month == i) & (df_temp.year == j)]))\n",
    "                    temp_min.append(np.mean(df_temp['min'][(df_temp.month == i) & (df_temp.year == j)]))\n",
    "                    temp_max.append(np.mean(df_temp['max'][(df_temp.month == i) & (df_temp.year == j)]))\n",
    "                year_months_mean.append(temp_mean) #& (df.year == j)])\n",
    "                year_months_min.append(temp_min) #mean (for the month) of the min values \n",
    "                year_months_max.append(temp_max) #mean (for the month) of the max values \n",
    "\n",
    "            months_mean = []\n",
    "            months_min = []\n",
    "            months_max = []\n",
    "            for i in range(0,12):\n",
    "                months_mean.append(round(np.mean(year_months_mean[i]),2))\n",
    "                months_min.append(round(np.mean(year_months_min[i]),2))\n",
    "                months_max.append(round(np.mean(year_months_max[i]),2))\n",
    "\n",
    "\n",
    "            #__________________________________________________________________________ERA5\n",
    "            ##   precipitation   precipitation   precipitation   precipitation\n",
    "\n",
    "            #use this to plot the whole dataset\n",
    "            data=[]\n",
    "            for i in np.unique(df_precip['month']):\n",
    "                temp=[]\n",
    "                for j in np.unique(df_precip['year']):\n",
    "                    temp.append(np.sum(df_precip['mean'][(df_precip['year']==j) & (df_precip['month']==i)]))\n",
    "                data.append(temp)\n",
    "            j = '{} - {}'.format(np.unique(df_precip['year'])[0],np.unique(df_precip['year'])[-1])\n",
    "\n",
    "            data_plot=[]\n",
    "            zero_value = False\n",
    "            for i in range(0,12):\n",
    "                if '0.0,' in str(data[i]):\n",
    "                    data_plot.append(np.sum(data[i])/(len(np.unique(df_precip['year']))-1))#lösche 0 wert!\n",
    "                    zero_value = True\n",
    "                else:\n",
    "                    data_plot.append(round(np.mean(data[i]),2))\n",
    "\n",
    "\n",
    "            #__________________________________________________________________________ERA5\n",
    "            ##   plot both   plot both   plot both   plot both   plot both   plot both  \n",
    "\n",
    "            if zero_value == False:\n",
    "                text = 'total mean: {} °C and {} mm'.format(round(np.mean(months_mean)),round(np.mean(data_plot)))\n",
    "                monate = ['Jan.','Feb.','Mar.','Apr.','May','June','July','Aug.','Sept.','Oct.','Nov.','Dec.']\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(9.5,5))\n",
    "                #fig.suptitle('Temperature and precipitation for the year(s):{}'.format(j),fontsize=16)\n",
    "\n",
    "                ax2 = ax.twinx()\n",
    "                ax.set_zorder(1)\n",
    "                ax.patch.set_visible(False)\n",
    "\n",
    "                ax.set_title('Temperature and precipitation for the year(s): \\n {}, lake ID: {}'.format(j, Lake_ID[0],fontsize=10))\n",
    "                ax.plot(monate, months_mean,'o', color='red', label='mean per month')# months_min,'-r',  months_max,'-r')\n",
    "                ax.plot(monate, months_max, linestyle='dashed', color='mediumpurple', label='mean of min per month')\n",
    "                ax.plot(monate, months_min, linestyle='dashed', color='cornflowerblue', label='mean of min per month')\n",
    "                ax.hlines(-0.225,-1,12,alpha=0.15)\n",
    "\n",
    "                ax.set_ylabel(\"Temp. in °C\")\n",
    "                ax.set_ylim([-40,25])\n",
    "\n",
    "                ax.set_xlabel(\"months\")\n",
    "\n",
    "\n",
    "                ax2.bar(monate, data_plot,alpha=0.35, label='mean of sum per month')\n",
    "                ax2.set_ylabel(\"PRCP. in mm\")\n",
    "                ax2.set_ylim([0,100])\n",
    "\n",
    "\n",
    "                leg = ax.legend(loc = 2, framealpha = 0.0,title=\"Temperature\",fontsize='small')#\n",
    "                leg._legend_box.align = \"left\"\n",
    "                leg2 = ax2.legend(loc = 1, framealpha = 0.0,title=\"Precipitation\",fontsize='small')#\n",
    "                leg2._legend_box.align = \"left\"\n",
    "                plt.text(8.7, 101.5, text,fontsize='small')\n",
    "\n",
    "                plt.xlim([-0.5,11.5])\n",
    "                #plt.show()\n",
    "                outfile = data_lake_path + '/climate_data_ID{}.jpg'.format(Lake_ID[0])\n",
    "                plt.savefig(outfile)\n",
    "                plt.close('all')\n",
    "\n",
    "            df_temp = []\n",
    "            df_precip = []\n",
    "            path_1 = []\n",
    "            path_2 = []\n",
    "            if zero_value == True:\n",
    "                #print('caution, 0 values found, Lake ID',Lake_ID[0])\n",
    "                zero_values_found.append(Lake_ID[0])\n",
    "\n",
    "print(len(zero_values_found),'lakes have no era5 plots')         \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0e2cbf-3955-4bab-aa6c-f4558251219b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    174952\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lake_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ddf13",
   "metadata": {},
   "source": [
    "-----\n",
    "### Some results\n",
    "----\n",
    "In 'data\\lake_data' now  the processed data for each lake of interest is stored. Those selected lakes experience a strong changing gradient (nt_ch_pc threshold). A changing climate is causing drastic impacts on permafrost dynamics, ecosystem functioning, biogeochemical processes, and human livelihoods in lowland permafrost regions. \n",
    "\n",
    "As one can see in the selected results below, different landcover classes are present for x particular lake/s, the diversity of those is described with the Index. In the Era5 data a gradient between lakes in the north - south can be seen. Therefore follow the steps for the visualisation of the genereated data in the CVL3D viewer below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28b99",
   "metadata": {},
   "source": [
    "- plot some example results here in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "filepaths = []\n",
    "lake_data_path = os.path.join('data','lake_data')\n",
    "for folder in os.listdir(lake_data_path):\n",
    "    for file in os.listdir(os.path.join(lake_data_path, folder)):\n",
    "        if 'landcover_classes_' in file and '.jpg' in file:\n",
    "            filepaths.append(os.path.join(lake_data_path, folder, file)) \n",
    "            c = c +1\n",
    "        if 'climate_data_' in file and '.jpg' in file:\n",
    "            filepaths.append(os.path.join(lake_data_path, folder, file))\n",
    "            c = c +1\n",
    "        if c == 2:\n",
    "            break\n",
    "    if c == 2:\n",
    "        break\n",
    "\n",
    "print('selected RESULTS:')\n",
    "print('...for Lake:', folder,'generated plots:')\n",
    "for file in filepaths:\n",
    "    display(Image(filename=file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e645c",
   "metadata": {},
   "source": [
    "### Visualisation of the genereated data in the the CVL3D viewer\n",
    "<em> You need to run the notebook now on your local machine! Until 24.01.2023 it was not possible within the polartep environment</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b64297",
   "metadata": {},
   "source": [
    "- tutorial about CVL3D viewer: https://github.com/CryosphereVirtualLab/cvl-3d-viz <br>\n",
    "   - <b>copy</b> the cvl folder from the [tutorial](https://github.com/CryosphereVirtualLab/cvl-3d-viz/tree/master/cvl) into the `selected_distrubed_lakes_ecosystem_data` directory <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8dd566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "module_path = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)# +\"/cvl\")\n",
    "\n",
    "#from cvl import viz\n",
    "sys.path\n",
    "\n",
    "from cvl.viz import viz, VBO, Raster\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import io\n",
    "from osgeo import osr\n",
    "from osgeo import gdal\n",
    "\n",
    "visualizer = viz()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74455e69",
   "metadata": {},
   "source": [
    "### starting the server\n",
    "in terminal (while rooted in `selected_distrubed_lakes_ecosystem_data` directory)\n",
    "- `openssl req -x509 -nodes -days 730 -newkey rsa:2048 -keyout key.pem -out cert.pem -config cvl/localhost-ssl.conf`\n",
    "- `python cvl/server.py`\n",
    "\n",
    "open https://localhost:3193/trust and trust the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31aee5",
   "metadata": {},
   "source": [
    "### display the shapefiles:\n",
    "- open https://cvl.eo.esa.int/ \n",
    "- browser issues: brave browser does NOT work, firefox works! (disable addons!)\n",
    "\n",
    "Is the server running? \n",
    "- follow tutorial cvl-3d-viz (see below) -> till 24.01.2023 not possible in the polartep... only if you run the notebook on your local machine!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4e95367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...display the shapefiles @ 3d viewer:\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "import shapefile\n",
    "path = 'data/lake_data'\n",
    "\n",
    "print('...display the shapefiles @ 3d viewer:')\n",
    "for folder in os.listdir(path):\n",
    "    if 'ID' in folder: \n",
    "        for files in os.listdir(path + '/' + folder):\n",
    "            if '.shp' in files:\n",
    "                file_input = path + '/' + folder + '/' + files\n",
    "                Lake_ID = files.split('_')[-1][0:-4]\n",
    "\n",
    "                geojson_data = shapefile.Reader(file_input).__geo_interface__\n",
    "                metadata = { \"path\" : \"shape_files\", \"geojson\" : geojson_data }\n",
    "                visualizer.publish_geojson('Lake: {}'.format(Lake_ID), metadata)\n",
    "print('...done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d723c86",
   "metadata": {},
   "source": [
    "The shapefiles of the lakes are now visible in the CVL3D as shown below:\n",
    "![](res/screenshot_1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd0b4e",
   "metadata": {},
   "source": [
    "- gen_index function:                                <em>(1 to 1 from cvl-3d-viz-master/notebooks/Examples)</em><br>\n",
    "    This function generates an index buffer rendering triangles. <br>\n",
    "    It assumes that the vertices are laid out as in a regular grid. <br>\n",
    "    Input parameters are the width and height of the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7117c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_index(width, height):\n",
    "    verts_per_line = 2*width\n",
    "    tris_per_line = verts_per_line-2\n",
    "    num_tris = tris_per_line*(height-1)\n",
    "    num_index = num_tris*3\n",
    "    indices = np.zeros((num_index), dtype=np.uint32)\n",
    "    idx = 0\n",
    "    for y in range(0, height-1):\n",
    "        for x in range(0, width-1):\n",
    "            indices[idx+0]\t= ((y+1) * width) + x\n",
    "            indices[idx+1]\t= (y*width)+x\n",
    "            indices[idx+2]\t= (y*width)+x+1\n",
    "            indices[idx+3]\t= (y*width)+x+1\n",
    "            indices[idx+4]\t= ((y+1) * width) + x+1\n",
    "            indices[idx+5]\t= ((y+1) * width) + x\n",
    "            idx += 6\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc4b25",
   "metadata": {},
   "source": [
    "- read the coordingates from the shapefile and display the data at the CVL3D:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb4daf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...done\n"
     ]
    }
   ],
   "source": [
    "import geopandas \n",
    "from pyproj import Proj\n",
    "\n",
    "#__________________________________________________________________________\n",
    "#display the available landcover and Era5 climate data \n",
    "def display_data(image_path, image_type):\n",
    "    if 'Landcover' in image_type:\n",
    "        image_origin = np.array([((origin[0])+250),((origin[1])+0),0])\n",
    "    if 'Climate Data' in image_type:\n",
    "        image_origin = np.array([((origin[0])+250),((origin[1])+700),0])\n",
    "\n",
    "    texcoords = np.zeros((4, 2), dtype=np.float32)\n",
    "    texcoords[0] = [0, 0]\n",
    "    texcoords[1] = [1, 0]\n",
    "    texcoords[2] = [0, 1]\n",
    "    texcoords[3] = [1, 1]\n",
    "    index = gen_index(2,2)\n",
    "\n",
    "    with open(image_path, \"rb\") as fd:\n",
    "        texture = fd.read()\n",
    "\n",
    "    #reposition of the plot\n",
    "    points = np.zeros((9,3), dtype=np.float64)\n",
    "    for y in range(0,3):\n",
    "        for x in range(0,3):\n",
    "            points[y*3+x] = image_origin+[500*x, 300*-y, 0]\n",
    "\n",
    "    raster = Raster(np.array(points[:, 0:2]), [3,3], 32633, image_data=texture)\n",
    "    metadata = { \"path\" : \"plots\" }\n",
    "    visualizer.publish_raster('{}: {}'.format(Lake_ID, image_type), metadata, raster)\n",
    "\n",
    "\n",
    "    \n",
    "#__________________________________________________________________________      \n",
    "\n",
    "#umrechnen!?:\n",
    "# gdf = geopandas.GeoDataFrame(df, geometry=gs, crs=\"EPSG:4326\")\n",
    "myProj = Proj(\"+proj=utm +zone=33 +north +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n",
    "\n",
    "#('...display the landcover plots and the era5 data  @ 3d viewer:')\n",
    "for folder in os.listdir(path):\n",
    "    if 'ID' in folder: \n",
    "        for files in os.listdir(path + '/' + folder):\n",
    "            if '.shp' in files:\n",
    "                file_input = path + '/' + folder + '/' + files\n",
    "                Lake_ID = files.split('_')[-1][0:-4]\n",
    "                \n",
    "                ds = geopandas.read_file(file_input)\n",
    "                ori = myProj(float(ds.bounds.maxx), float(ds.bounds.maxy))\n",
    "                origin = np.array([list(ori)[0],list(ori)[1],0])\n",
    "                \n",
    "                for file in os.listdir(path + '/' + folder):\n",
    "                    if '.jpg'in file and 'landcover' in file:\n",
    "                        image_path = path + '/' + folder + '/' + file\n",
    "                        image_type = 'Landcover'\n",
    "                        display_data(image_path, image_type)\n",
    "                    if '.jpg'in file and 'climate_data' in file:\n",
    "                        image_path = path + '/' + folder + '/' + file\n",
    "                        image_type = 'Climate Data'\n",
    "                        display_data(image_path, image_type)\n",
    "\n",
    "print('...done')     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4b3e4",
   "metadata": {},
   "source": [
    "The <em>Landcover</em> and available <em>\"Climate Data\"</em> of the lakes are now added in the CVL3D as shown below:\n",
    "![](res/screenshot_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ca009",
   "metadata": {},
   "source": [
    "- expected output: <em>see screenshots below</em>\n",
    "<br>zoom in to one lake: \n",
    "![](res/screenshot_04.PNG)\n",
    "<br>zoom in to another lake:\n",
    "![](res/screenshot_5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983e6a8",
   "metadata": {},
   "source": [
    "- important: <br>\n",
    "    - in the <em> \"Background Layer\" </em>section: <em> \"Vector Layer\" </em>needs to be <b>enabled!</b> <em> Screenshot below, otherwise the <b>!whole!</b> data is not visible! </em> <br>\n",
    "    - the image <b>tilt</b> need to be investigated...\n",
    "![](res/screenshot_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abe241",
   "metadata": {},
   "source": [
    "The dataset is now visible at the Cryosphere Virtual Laboratory 3D Viewer, providing visualization and navigation of the georeferenced data. This includes the calculated indices, the pressure and temperature data and the different landcover classes for the selected lake areas. \n",
    "\n",
    "The changing cryosphere focusing on disturbed lakes can be described with the produced data. This includes ecosystem data (landcover classes) and lake area change (trend data). \n",
    "\n",
    "<em>The processing was done in this notebook by combining and editing multi-source data sets within the CVL environment.</em>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
