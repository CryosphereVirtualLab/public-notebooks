{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60e689ae-b00a-48c4-88d0-e8f1285774d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecosystem data for selected disturbed lakes in permafrost regions \n",
      " by Clemens von Baeckmann\n"
     ]
    }
   ],
   "source": [
    "print('Ecosystem data for selected disturbed lakes in permafrost regions \\n by Clemens von Baeckmann')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e6ecf34-09ac-4b1b-a446-699ede734e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you need to download era5  data? y or n?\n",
      "download_data: False\n",
      "era5 data: False\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import zipfile\n",
    "import pygrib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio import mask as msk \n",
    "from matplotlib import pyplot as plt  \n",
    "from osgeo import gdal, ogr\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "download_data = False # insert False or True\n",
    "buffer_dist = 30 # define buffer dist here (CVL proposal 30 meter)\n",
    "buffer_dist_era5 = 10000 #define era5 buffer dist around the lake, must be huge to have data! era5 data grid!!)\n",
    "Lakes_url = 'https://hs.pangaea.de/sat/NitzeI-etal_2018/Lakes_T1.zip'\n",
    "Landcover_url = 'https://hs.pangaea.de/sat/GlobPermafrost_Sentinel/Landcover/ZAM_LCP_LANDC_SEN12_V02_20150815_20180830_T01.tif'\n",
    "\n",
    "# include ERA5 Data for selected area!\n",
    "print('do you need to download era5  data? y or n?')\n",
    "answer = 'n'\n",
    "#answer = input()\n",
    "if answer == 'y':\n",
    "    decision = True\n",
    "if answer == 'n':\n",
    "    decision = False\n",
    "need_to_download_era5 = decision\n",
    "\n",
    "\n",
    "print('download_data:', download_data)\n",
    "print('era5 data:',need_to_download_era5)\n",
    "\n",
    "if os.path.exists('data') == False:\n",
    "    os.makedirs('data')\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf786a37-52ff-4c19-8340-60b3ac8ebf80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait till ALL datasets are loaded...\n",
      "ds_landcover loaded\n",
      "ds_trend loaded\n",
      "ALL main datasets are loaded!\n"
     ]
    }
   ],
   "source": [
    "#download& read data\n",
    "print('wait till ALL datasets are loaded...')\n",
    "\n",
    "if download_data == True:\n",
    "    print('___loading Landcover')\n",
    "    \n",
    "    if os.path.exists('data/ds_landcover.tif') == False:\n",
    "        download_string = 'wget -O data/ds_landcover.tif {}'.format(Landcover_url)\n",
    "        os.system(download_string)\n",
    "    ds_landcover =  rio.open(Landcover_url)\n",
    "\n",
    "    print('___loading Trends')\n",
    "    if os.path.exists('data/Lakes_T1/T1_WS_Lakes.shp') == False:\n",
    "        print ('downloading and open dataset trend-Lakes')\n",
    "        download_string = 'wget -O data/Lakes_T1.zip {}'.format(Lakes_url) \n",
    "        os.system(download_string)\n",
    "        with zipfile.ZipFile('data/Lakes_T1.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "        print('data downloaded')\n",
    "\n",
    "    ds_trend = gpd.read_file('data/Lakes_T1/T1_WS_Lakes.shp')          \n",
    "    print('dataset loaded')\n",
    "    \n",
    "else:\n",
    "    ds_landcover = rio.open('data/ds_landcover.tif')\n",
    "    print('ds_landcover loaded')\n",
    "    ds_trend = gpd.read_file('data/Lakes_T1/T1_WS_Lakes.shp')   \n",
    "    print('ds_trend loaded')\n",
    "\n",
    "print('ALL main datasets are loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "576ab2f8-9ad7-4daa-b9c3-282514f2b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 lakes are loaded\n",
      "nt_ch_pc is set to: -99\n"
     ]
    }
   ],
   "source": [
    "#lakes of interest = nt_ch_pc = - and größer als zb -90 percent\n",
    "nt_ch_pc_value = -99\n",
    "Lakes_OI = copy.deepcopy(ds_trend[ds_trend.nt_ch_pc <= nt_ch_pc_value]).reset_index(drop=True)\n",
    "print('{} lakes are loaded'.format(len(Lakes_OI)))\n",
    "print('nt_ch_pc is set to: {}'.format(nt_ch_pc_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a365d4be-67f6-48fe-a8ac-5aa311c99ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut landcover with shape\n",
      "doing 115 lakes\n",
      "errors with 172200\n",
      "errors with 172343\n",
      "errors with 172388\n",
      "errors with 172504\n",
      "errors with 173070\n",
      "errors with 173681\n",
      "errors with 174085\n",
      "errors with 174550\n",
      "errors with 174600\n",
      "errors with 174675\n",
      "errors with 174720\n",
      "errors with 174848\n",
      "errors with 174884\n",
      "errors with 174937\n",
      "errors with 174952\n",
      "problems with 15 lakes\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#cut now the landcover for the particular lake\n",
    "print('cut landcover with shape')\n",
    "print( 'doing {} lakes'.format(len(Lakes_OI.index)))\n",
    "\n",
    "count_rm = 0\n",
    "for i in Lakes_OI.index:\n",
    "    #Lake_ID = Lakes_OI.ID[i]\n",
    "    Lake = copy.deepcopy(Lakes_OI[Lakes_OI.index == i]).reset_index(drop = True)\n",
    "    Lake_ID = Lake.ID\n",
    "\n",
    "    #generate now for each lake an folder with all the calculated values:\n",
    "    data_lake_path = 'data/lake_data/ID{}'.format(np.array(Lake_ID)[0])\n",
    "    if os.path.exists(data_lake_path) == False:\n",
    "        os.makedirs(data_lake_path)\n",
    "        #print(Lake_ID)\n",
    "        #print('masking now the landcover with lake {}'.format(np.array(Lake_ID)[0]))\n",
    "\n",
    "        try: \n",
    "            Lake = Lake.to_crs(crs = 32642)\n",
    "            Lake.geometry = copy.deepcopy(Lake.buffer(buffer_dist,join_style=1))\n",
    "\n",
    "            out_image, out_transform = msk.mask(ds_landcover, Lake.geometry, crop=True)\n",
    "            out_meta = ds_landcover.meta\n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": out_image.shape[1],\n",
    "                             \"width\": out_image.shape[2],\n",
    "                             \"transform\": out_transform})\n",
    "\n",
    "            write_tif = '{}/ID{}.tif'.format(data_lake_path,np.array(Lake_ID)[0])\n",
    "\n",
    "            with rio.open(write_tif, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "                \n",
    "        except:\n",
    "            print('errors with', np.array(Lake_ID)[0])\n",
    "            count_rm = count_rm + 1\n",
    "\n",
    "print('problems with', count_rm, 'lakes')\n",
    "print('done')\n",
    "\n",
    "#to remove all generated files:\n",
    "#rm 'data/lake_data' -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb951f8-4fc9-44a8-820e-8f42ac60cbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you realy want to delete all empty files? \n"
     ]
    }
   ],
   "source": [
    "#delete all empty files\n",
    "input('do you realy want to delete all empty files?')\n",
    "for folder in os.listdir('data/lake_data'):\n",
    "    search = 'data/lake_data/{}'.format(folder)\n",
    "    if not os.listdir(search):\n",
    "        os.rmdir(search)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe03c539-90c1-4a4f-90ad-0e4cc67ccc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you need to download the additional era5 data, here is the file. you need to register and intall the cdsapi python package \n",
      " or you download it manualy: reanalysis-era5-single-levels, precipitation and in a second step temperature \n",
      "data need to be stored in /data/era5\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#in case u want to download the additional era5 data: \n",
    "#recommended: use the existing one, downloading can take very long and cause troubles\n",
    "#\n",
    "print('if you need to download the additional era5 data, here is the file. you need to register and intall the cdsapi python package \\n or you download it manualy: reanalysis-era5-single-levels, precipitation and in a second step temperature ')\n",
    "print('data need to be stored in /data/era5')\n",
    "\n",
    "if need_to_download_era5 == True:\n",
    "    import cdsapi\n",
    "    \n",
    "    print('download the total_precipitation data')\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'format': 'grib',\n",
    "            'variable': 'total_precipitation',\n",
    "            'year': [\n",
    "                '2018', '2019', '2020',\n",
    "                '2021',\n",
    "            ],\n",
    "            'month': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "            ],\n",
    "            'day': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "            ],\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00',\n",
    "                '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00',\n",
    "                '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00',\n",
    "                '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00',\n",
    "                '21:00', '22:00', '23:00',\n",
    "            ],\n",
    "            'area': [\n",
    "                74, 65, 65,\n",
    "                74,\n",
    "            ],\n",
    "        },\n",
    "        '/data/era5/yamal_precip.grib')\n",
    "\n",
    "\n",
    "    #and 2m_temperature data\n",
    "\n",
    "    print('download the 2m_temperature data')\n",
    "    c = cdsapi.Client()\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'format': 'grib',\n",
    "            'variable': '2m_temperature',\n",
    "            'year': [\n",
    "                '2018', '2019', '2020',\n",
    "                '2021',\n",
    "            ],\n",
    "            'month': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "            ],\n",
    "            'day': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "            ],\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00',\n",
    "                '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00',\n",
    "                '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00',\n",
    "                '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00',\n",
    "                '21:00', '22:00', '23:00',\n",
    "            ],\n",
    "            'area': [\n",
    "                74, 65, 65,\n",
    "                74,\n",
    "            ],\n",
    "        },\n",
    "        '/data/era5/yamal_temp.grib')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7e5e06f-2600-4230-8872-486cf0c8f166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you realy want to cut for each lake the era5 data? y or n?\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#cut now the era5 data for the particular lake\n",
    "print('do you realy want to cut for each lake the era5 data? y or n?')\n",
    "answer = 'n'\n",
    "#answer = input()\n",
    "if answer == 'y':\n",
    "    decision = True\n",
    "if answer == 'n':\n",
    "    decision = False\n",
    "\n",
    "if decision:\n",
    "    print('continue... please wait...')\n",
    "    for i in Lakes_OI.index:\n",
    "        print(i+1, '/', len(Lakes_OI))\n",
    "        #Lake_ID = Lakes_OI.ID[i]\n",
    "        Lake = copy.deepcopy(Lakes_OI[Lakes_OI.index == i]).reset_index(drop = True)\n",
    "        Lake = Lake.to_crs(crs = 32642)\n",
    "        Lake_ID = Lake.ID\n",
    "\n",
    "        data_lake_path = 'data/lake_data/ID{}'.format(np.array(Lake_ID)[0])\n",
    "\n",
    "        if os.path.exists(data_lake_path) == True:\n",
    "            data = r'data/era5'\n",
    "            cut_data = copy.deepcopy(Lake)\n",
    "            cut_data.geometry = copy.deepcopy(Lake.buffer(buffer_dist_era5,join_style=1))\n",
    "            begin_of_dataset = 2015\n",
    "\n",
    "            data_paths = []\n",
    "            for root, dirs, files in os.walk(data):\n",
    "                    for d in files:\n",
    "                        if d.endswith('.grib'):\n",
    "                            data_paths.append(root+'/'+d)\n",
    "\n",
    "            last_year = 0\n",
    "            last_month = 0\n",
    "            for i in range(0,len(data_paths)):\n",
    "                print('reading data')\n",
    "                print(data_paths[i])\n",
    "                grbs = pygrib.open(data_paths[i])\n",
    "\n",
    "                date = []\n",
    "                mean = []\n",
    "                median = []\n",
    "                min_ = []\n",
    "                max_ = []\n",
    "                year = []\n",
    "                month = []\n",
    "                day = []\n",
    "\n",
    "                for grb in grbs.select():\n",
    "                    outfile = '{}/era5_{}_ID{}.pkl'.format(data_lake_path, grb.parameterName,np.array(Lake_ID)[0])\n",
    "\n",
    "                if not os.path.isfile(outfile):\n",
    "                    print('masking .grib file to extend in the defining shapefile')\n",
    "                    print('using dataset since:',begin_of_dataset)\n",
    "\n",
    "                    for grb in grbs.select():\n",
    "\n",
    "                        #input(grb.keys()) \n",
    "                        # for i in grb.keys():\n",
    "                        #     try:\n",
    "                        #         output = str(grb[i])\n",
    "                        #     except:\n",
    "                        #         output = '_nan_'\n",
    "                        #     print('\\n{}\\t\\t{}'.format(i,output))\n",
    "\n",
    "                        data = grb.latLonValues\n",
    "                        ts = str(grb.julianDay).replace('.','_') \n",
    "                        write = False\n",
    "                        if grb.year >= begin_of_dataset:\n",
    "                            write = True\n",
    "                            #if not grb.year == last_year:\n",
    "                            #    print('Year:',grb.year)\n",
    "                            #last_year = grb.year\n",
    "\n",
    "                            #if not grb.month == last_month:\n",
    "                            #    print('Month:',grb.month)\n",
    "                            #last_month = grb.month\n",
    "\n",
    "\n",
    "                            lat_lon_val = ([],[],[])\n",
    "                            u = 0\n",
    "                            for j in range(0,(len(data))):\n",
    "                                k = 0\n",
    "                                if u == 2:\n",
    "                                    u = 0\n",
    "                                    k = 1\n",
    "                                    if 'temperature' in grb.parameterName:\n",
    "                                        lat_lon_val[2].append(data[j]-273.15) #unit K to °C\n",
    "                                        scale = 'Grad Celsius'\n",
    "                                    if 'precipitation' in grb.parameterName: \n",
    "                                        lat_lon_val[2].append(data[j]*1000)  #unit m to mm  \n",
    "                                        scale = 'Millimeter'\n",
    "                                if u == 1:\n",
    "                                    u = u + 1\n",
    "                                    lat_lon_val[1].append(data[j])\n",
    "                                if u == 0 and k == 0:\n",
    "                                    u = u + 1\n",
    "                                    lat_lon_val[0].append(data[j])\n",
    "\n",
    "                            lat_lon_val =np.array(lat_lon_val)\n",
    "                            lat_lon_val=lat_lon_val.transpose()\n",
    "\n",
    "                            pointDf = pd.DataFrame(lat_lon_val)\n",
    "                            pointDf.rename( columns={2 :'values'}, inplace=True )\n",
    "\n",
    "                            gdf = gpd.GeoDataFrame(pointDf['values'],geometry=gpd.points_from_xy(pointDf[1],pointDf[0]),crs=4326)\n",
    "                            #cut_data = gpd.read_file(cut_shape)\n",
    "\n",
    "                            cut_data = copy.deepcopy(cut_data.to_crs(4326))\n",
    "                            gdf = gdf.to_crs(4326) \n",
    "\n",
    "                            #df1 = gpd.read_file(r'D:\\Yamal\\temp_precipitation_plot\\cropPoints1.shp')\n",
    "                            res_intersection = gdf.overlay(cut_data, how='intersection')\n",
    "                            if 'id' in res_intersection.columns:\n",
    "                                del res_intersection['id']\n",
    "\n",
    "                            #want to check the correct cutted extend:\n",
    "                            #res_intersection.to_file(r'D:\\Yamal\\data2\\Landcover_self_generated\\ID45136\\test.shp', driver='ESRI Shapefile')\n",
    "\n",
    "                            date.append(grb.julianDay)\n",
    "                            mean.append(np.mean(res_intersection['values']))\n",
    "                            median.append(np.median(res_intersection['values']))\n",
    "                            min_.append(np.min(res_intersection['values']))\n",
    "                            max_.append(np.max(res_intersection['values']))\n",
    "                            year.append(grb.year)\n",
    "                            month.append(grb.month)\n",
    "                            day.append(grb.day)\n",
    "\n",
    "                    print('generating and writing the masked database', grb.parameterName,' to main folder')\n",
    "                    database = pd.DataFrame({'ID':grb.parameterName,\n",
    "                                             'date':date,\n",
    "                                             'year':year,\n",
    "                                             'month':month,\n",
    "                                             'day':day,\n",
    "                                             'mean':mean,\n",
    "                                             'median':median,\n",
    "                                             'min':min_,\n",
    "                                             'max':max_})\n",
    "\n",
    "                    outfile = '{}/era5_{}_ID{}.csv'.format(data_lake_path, grb.parameterName,np.array(Lake_ID)[0])\n",
    "                    database.to_csv(outfile)  # where to save it, usually as a .pkl\n",
    "                    print(outfile)\n",
    "\n",
    "    print('done with era5 data cutting')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1de5f17e-d0a6-4884-b9b2-0d60715ea928",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89696453-2248-449d-9ce4-20592f10fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate now the era5 plots for each lake\n",
      "please wait...\n",
      "34 lakes have no era5 plots\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('generate now the era5 plots for each lake')\n",
    "print('please wait...')\n",
    "df_temp = []\n",
    "df_precip = []\n",
    "zero_values_found = []\n",
    "for i in Lakes_OI.index:\n",
    "    #print(i+1, '/', len(Lakes_OI))\n",
    "    Lake = copy.deepcopy(Lakes_OI[Lakes_OI.index == i]).reset_index(drop = True)\n",
    "    Lake = Lake.to_crs(crs = 32642)\n",
    "    Lake_ID = Lake.ID\n",
    "    data_lake_path = 'data/lake_data/ID{}'.format(np.array(Lake_ID)[0])\n",
    "    path_1 = []\n",
    "    path_2 = []\n",
    "    run = False\n",
    "    if os.path.exists(data_lake_path) == True:\n",
    "        for file in os.listdir(data_lake_path):\n",
    "            if '.csv' in file: \n",
    "                if 'temperature' in file:\n",
    "                    path_1 = data_lake_path + '/' + file\n",
    "                    df_temp = pd.read_csv(path_1)\n",
    "                    run = True\n",
    "                if 'precipitation' in file:\n",
    "                    path_2 = data_lake_path + '/' + file\n",
    "                    df_precip = pd.read_csv(path_2)\n",
    "                    run = True\n",
    "\n",
    "        #df_temp = pd.read_pickle(path_1)\n",
    "        #df_precip = pd.read_pickle(path_2)\n",
    "        \n",
    "        if run == True:\n",
    "            c=0\n",
    "\n",
    "            #__________________________________________________________________________ERA5\n",
    "            ##     temperature      temperature       temperature       temperature\n",
    "\n",
    "            #daten aufbereiten\n",
    "            year_months_mean = []\n",
    "            year_months_min = []\n",
    "            year_months_max = []\n",
    "\n",
    "            for i in np.unique(df_temp.month):\n",
    "                temp_mean=[]\n",
    "                temp_min=[]\n",
    "                temp_max=[]\n",
    "                for j in np.unique(df_temp.year):\n",
    "                    temp_mean.append(np.mean(df_temp['mean'][(df_temp.month == i) & (df_temp.year == j)]))\n",
    "                    temp_min.append(np.mean(df_temp['min'][(df_temp.month == i) & (df_temp.year == j)]))\n",
    "                    temp_max.append(np.mean(df_temp['max'][(df_temp.month == i) & (df_temp.year == j)]))\n",
    "                year_months_mean.append(temp_mean) #& (df.year == j)])\n",
    "                year_months_min.append(temp_min) #mean (for the month) of the min values \n",
    "                year_months_max.append(temp_max) #mean (for the month) of the max values \n",
    "\n",
    "            months_mean = []\n",
    "            months_min = []\n",
    "            months_max = []\n",
    "            for i in range(0,12):\n",
    "                months_mean.append(round(np.mean(year_months_mean[i]),2))\n",
    "                months_min.append(round(np.mean(year_months_min[i]),2))\n",
    "                months_max.append(round(np.mean(year_months_max[i]),2))\n",
    "\n",
    "\n",
    "            #__________________________________________________________________________ERA5\n",
    "            ##   precipitation   precipitation   precipitation   precipitation\n",
    "\n",
    "            #use this to plot the whole dataset\n",
    "            data=[]\n",
    "            for i in np.unique(df_precip['month']):\n",
    "                temp=[]\n",
    "                for j in np.unique(df_precip['year']):\n",
    "                    temp.append(np.sum(df_precip['mean'][(df_precip['year']==j) & (df_precip['month']==i)]))\n",
    "                data.append(temp)\n",
    "            j = '{} - {}'.format(np.unique(df_precip['year'])[0],np.unique(df_precip['year'])[-1])\n",
    "\n",
    "            data_plot=[]\n",
    "            zero_value = False\n",
    "            for i in range(0,12):\n",
    "                if '0.0,' in str(data[i]):\n",
    "                    data_plot.append(np.sum(data[i])/(len(np.unique(df_precip['year']))-1))#lösche 0 wert!\n",
    "                    zero_value = True\n",
    "                else:\n",
    "                    data_plot.append(round(np.mean(data[i]),2))\n",
    "\n",
    "\n",
    "            #__________________________________________________________________________ERA5\n",
    "            ##   plot both   plot both   plot both   plot both   plot both   plot both  \n",
    "\n",
    "            if zero_value == False:\n",
    "                text = 'total mean: {} °C and {} mm'.format(round(np.mean(months_mean)),round(np.mean(data_plot)))\n",
    "                monate = ['Jan.','Feb.','Mar.','Apr.','May','June','July','Aug.','Sept.','Oct.','Nov.','Dec.']\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(9.5,5))\n",
    "                #fig.suptitle('Temperature and precipitation for the year(s):{}'.format(j),fontsize=16)\n",
    "\n",
    "                ax2 = ax.twinx()\n",
    "                ax.set_zorder(1)\n",
    "                ax.patch.set_visible(False)\n",
    "\n",
    "                ax.set_title('Temperature and precipitation for the year(s): \\n {}, lake ID: {}'.format(j, Lake_ID[0],fontsize=10))\n",
    "                ax.plot(monate, months_mean,'o', color='red', label='mean per month')# months_min,'-r',  months_max,'-r')\n",
    "                ax.plot(monate, months_max, linestyle='dashed', color='mediumpurple', label='mean of min per month')\n",
    "                ax.plot(monate, months_min, linestyle='dashed', color='cornflowerblue', label='mean of min per month')\n",
    "                ax.hlines(-0.225,-1,12,alpha=0.15)\n",
    "\n",
    "                ax.set_ylabel(\"Temp. in °C\")\n",
    "                ax.set_ylim([-40,25])\n",
    "\n",
    "                ax.set_xlabel(\"months\")\n",
    "\n",
    "\n",
    "                ax2.bar(monate, data_plot,alpha=0.35, label='mean of sum per month')\n",
    "                ax2.set_ylabel(\"PRCP. in mm\")\n",
    "                ax2.set_ylim([0,100])\n",
    "\n",
    "\n",
    "                leg = ax.legend(loc = 2, framealpha = 0.0,title=\"Temperature\",fontsize='small')#\n",
    "                leg._legend_box.align = \"left\"\n",
    "                leg2 = ax2.legend(loc = 1, framealpha = 0.0,title=\"Precipitation\",fontsize='small')#\n",
    "                leg2._legend_box.align = \"left\"\n",
    "                plt.text(8.7, 101.5, text,fontsize='small')\n",
    "\n",
    "                plt.xlim([-0.5,11.5])\n",
    "                #plt.show()\n",
    "                outfile = data_lake_path + '/climate_data_ID{}.jpg'.format(Lake_ID[0])\n",
    "                plt.savefig(outfile)\n",
    "                plt.close('all')\n",
    "\n",
    "            df_temp = []\n",
    "            df_precip = []\n",
    "            path_1 = []\n",
    "            path_2 = []\n",
    "            if zero_value == True:\n",
    "                #print('caution, 0 values found, Lake ID',Lake_ID[0])\n",
    "                zero_values_found.append(Lake_ID[0])\n",
    "\n",
    "print(len(zero_values_found),'lakes have no era5 plots')         \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57258eeb-f84b-4917-b435-4df8e4665c02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation of the existing data... calc indices... calc landcover distribution...\n",
      "please wait...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#cut now the landcover with this particular lake\n",
    "print('evaluation of the existing data... calc indices... calc landcover distribution...')\n",
    "print('please wait...')\n",
    "\n",
    "for i in Lakes_OI.index:\n",
    "    #Lake_ID = Lakes_OI.ID[i]\n",
    "    Lake = copy.deepcopy(Lakes_OI[Lakes_OI.index == i]).reset_index(drop = True)\n",
    "    Lake_ID = Lake.ID\n",
    "\n",
    "    #print(Lake_ID[0])\n",
    "    data_lake_path = 'data/lake_data/ID{}'.format(np.array(Lake_ID)[0])\n",
    "    \n",
    "    if os.path.exists(data_lake_path) == True:\n",
    "        for file in os.listdir(data_lake_path):\n",
    "            if 'ID' and '.tif' in file:\n",
    "                infile = data_lake_path + '/' + file\n",
    "    \n",
    "        #analyze the cutted lake\n",
    "        ds = gdal.Open(infile, gdal.GA_ReadOnly)\n",
    "        b_ds = ds.GetRasterBand(1) \n",
    "        b_ds_data = b_ds.ReadAsArray().astype(np.float32)\n",
    "\n",
    "        #make the nan values\n",
    "        dimension = np.shape(b_ds_data)\n",
    "        db_image = copy.deepcopy(b_ds_data)\n",
    "        for i in range(dimension[0]):\n",
    "            for j in range(dimension[1]):\n",
    "                if b_ds_data[i,j] == 255:\n",
    "                    db_image[i,j]=np.nan\n",
    "\n",
    "        #prepare the data\n",
    "        classes, count = np.unique(db_image,return_counts=True)\n",
    "        tot_sum = np.sum(count[0:(len(count)-1)]) #last value from count is count of nan values which is negligible\n",
    "        real_classes = real_classes = range(1,22)\n",
    "\n",
    "        labels = ['Sparse vegetation',\n",
    "                  'Sparse vegetation',\n",
    "                  'Shrub tundra',\n",
    "                  'Shrub tundra',\n",
    "                  'Shrub tundra',\n",
    "                  'Shrub tundra',\n",
    "                  'Shrub tundra',\n",
    "                  'Forest',\n",
    "                  'Forest',\n",
    "                  'Forest',\n",
    "                  'Grassland',\n",
    "                  'Floodplain',\n",
    "                  'Disturbed',\n",
    "                  'Floodplain',\n",
    "                  'Floodplain',\n",
    "                  'Floodplain',\n",
    "                  'Barren',\n",
    "                  'Barren',\n",
    "                  'Water',\n",
    "                  'Water',\n",
    "                  'Water']\n",
    "        \n",
    "        #__________________________________________________________________________\n",
    "        #calc main classes     #calc main classes     #calc main classes  \n",
    "        \n",
    "        data = []\n",
    "        for i in real_classes:\n",
    "            data.append(i)\n",
    "        data = pd.DataFrame({'classes':data})\n",
    "        labels = pd.DataFrame(labels)\n",
    "        df = data.join(labels)\n",
    "        dataframe = pd.DataFrame({'classes':classes[0:-1], 'count':count[0:-1]})\n",
    "        class_data = df.set_index('classes').join(dataframe.set_index('classes'))\n",
    "        class_data = class_data.reset_index()\n",
    "        three_main_classes = class_data.sort_values(by=['count'],ascending=False)[0:3]\n",
    "        #print('main classes are:',list(three_main_classes[0]))\n",
    "        Lake['main_class'] = str(list(three_main_classes[0]))\n",
    "\n",
    "        #percent\n",
    "        pixel_summe = np.sum(count[0:-1]) #exclude nan count!\n",
    "        prozente = (count[0:-1]/pixel_summe)*100\n",
    "        present_classes = len(classes[0:-1])\n",
    "        #print('present_classes:', present_classes)\n",
    "\n",
    "        #__________________________________________________________________________\n",
    "        #diversity index     diversity index     diversity index     diversity index\n",
    "        \n",
    "        #1 = divers 0 = mono\n",
    "        n_classes = len(dataframe)\n",
    "        # Diversity_in_classes_Index\n",
    "        DIC_I = n_classes / len(real_classes)\n",
    "        temp = class_data.sort_values(by=['count'],ascending=False)\n",
    "        temp = list(temp['count'])\n",
    "        pixel_summe = np.nansum(temp) \n",
    "                \n",
    "        #verteilung\n",
    "        verteilung = []\n",
    "        for i in temp:\n",
    "            verteilung.append((i / pixel_summe)**2) \n",
    "        #https://de.wikipedia.org/wiki/Simpson-Index\n",
    "        #https://www.researchgate.net/publication/311506613_Diversity_erfassen_Statistische_Diversitatsindizes\n",
    "\n",
    "        Div_index = 1 - np.nansum(verteilung)\n",
    "        Div_index = np.round(Div_index,3)\n",
    "        Lake['div_index'] = Div_index\n",
    "        #print('diversity index:', Div_index)\n",
    "        \n",
    "        #safe .shp file\n",
    "        outfile = data_lake_path + '/shape_ID{}.shp'.format(Lake_ID[0])\n",
    "        Lake.to_file(outfile)  \n",
    "        \n",
    "        #__________________________________________________________________________\n",
    "        #histogram     histogram     histogram     histogram     histogram\n",
    "        \n",
    "        outfile = data_lake_path + '/landcover_classes_ID{}.jpg'.format(Lake_ID[0])\n",
    "        plt.title('Landcover classes for Lake: {}'.format(Lake_ID[0]))\n",
    "        plt.bar(classes[0:(len(count)-1)],count[0:(len(count)-1)])\n",
    "        plt.xlabel('Landcover classes')\n",
    "        plt.ylabel('n of classified Pixel')\n",
    "        plt.xticks(real_classes)\n",
    "        \n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        plt.text(0,0.85,'Diversity Index: {}'.format(Div_index),\n",
    "                 fontsize='xx-large',\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom',\n",
    "                 transform=ax.transAxes,\n",
    "                bbox=props)\n",
    "        \n",
    "        plt.savefig(outfile)\n",
    "        plt.close('all')\n",
    "        #print('safed_something')\n",
    "        #input('press any key to continue')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8dd566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the cvl-rd-viz you need the cvl folder...\n",
      "you need to add the folder into your sys.path to import viz, VBO and Raster\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#visualize a lake in 3d viewer\n",
    "print('from the cvl-rd-viz you need the cvl folder...')\n",
    "\n",
    "#first implement the data in the CVL3D viewer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print('you need to add the folder into your sys.path to import viz, VBO and Raster')\n",
    "\n",
    "module_path = 'C:\\\\Users\\\\baeckmann\\\\Documents\\\\cvl_notebooks\\\\cvl-3d-viz-master\\\\'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)#+\"/cvl\"\n",
    "\n",
    "#from cvl import viz\n",
    "sys.path\n",
    "\n",
    "from cvl.viz import viz, VBO, Raster\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import io\n",
    "from osgeo import osr\n",
    "from osgeo import gdal\n",
    "\n",
    "visualizer = viz()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4e95367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the server running? follow tutorial cvl-3d-viz -> till 24.01.2023 not possible in the polartep... only if you run the notebook on your local machine!\n",
      "...display the shapefiles @ 3d viewer:\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "# display the shapefiles\n",
    "print('is the server running? follow tutorial cvl-3d-viz -> till 24.01.2023 not possible in the polartep... only if you run the notebook on your local machine!')\n",
    "import shapefile\n",
    "path = 'data/lake_data'\n",
    "\n",
    "print('...display the shapefiles @ 3d viewer:')\n",
    "for folder in os.listdir(path):\n",
    "    if 'ID' in folder: \n",
    "        for files in os.listdir(path + '/' + folder):\n",
    "            if '.shp' in files:\n",
    "                file_input = path + '/' + folder + '/' + files\n",
    "                Lake_ID = files.split('_')[-1][0:-4]\n",
    "\n",
    "                geojson_data = shapefile.Reader(file_input).__geo_interface__\n",
    "                metadata = { \"path\" : \"shape_files\", \"geojson\" : geojson_data }\n",
    "                visualizer.publish_geojson('Lake: {}'.format(Lake_ID), metadata)\n",
    "print('...done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7117c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_index                                                          (1 to 1 from cvl-3d-viz-master/notebooks/Examples)\n",
    "#This function generates an index buffer rendering triangles. \n",
    "#It assumes that the vertices are laid out as in a regular grid. \n",
    "#Input parameters are the width and height of the grid.\n",
    "\n",
    "def gen_index(width, height):\n",
    "    verts_per_line = 2*width\n",
    "    tris_per_line = verts_per_line-2\n",
    "    num_tris = tris_per_line*(height-1)\n",
    "    num_index = num_tris*3\n",
    "    indices = np.zeros((num_index), dtype=np.uint32)\n",
    "    idx = 0\n",
    "    for y in range(0, height-1):\n",
    "        for x in range(0, width-1):\n",
    "            indices[idx+0]\t= ((y+1) * width) + x\n",
    "            indices[idx+1]\t= (y*width)+x\n",
    "            indices[idx+2]\t= (y*width)+x+1\n",
    "            indices[idx+3]\t= (y*width)+x+1\n",
    "            indices[idx+4]\t= ((y+1) * width) + x+1\n",
    "            indices[idx+5]\t= ((y+1) * width) + x\n",
    "            idx += 6\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb4daf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...display the landcover plots and the era5 data  @ 3d viewer:\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "#read the coordingates from the shapefile and display the data\n",
    "\n",
    "import geopandas \n",
    "from pyproj import Proj\n",
    "\n",
    "#__________________________________________________________________________\n",
    "#display the available landcover and Era5 climate data \n",
    "def diplay_data(image_path, image_type):\n",
    "    if 'Landcover' in image_type:\n",
    "        image_origin = np.array([((origin[0])+250),((origin[1])+0),0])\n",
    "    if 'Climate Data' in image_type:\n",
    "        image_origin = np.array([((origin[0])+250),((origin[1])+700),0])\n",
    "\n",
    "    texcoords = np.zeros((4, 2), dtype=np.float32)\n",
    "    texcoords[0] = [0, 0]\n",
    "    texcoords[1] = [1, 0]\n",
    "    texcoords[2] = [0, 1]\n",
    "    texcoords[3] = [1, 1]\n",
    "    index = gen_index(2,2)\n",
    "\n",
    "    with open(image_path, \"rb\") as fd:\n",
    "        texture = fd.read()\n",
    "\n",
    "    #reposition of the plot\n",
    "    points = np.zeros((9,3), dtype=np.float64)\n",
    "    for y in range(0,3):\n",
    "        for x in range(0,3):\n",
    "            points[y*3+x] = image_origin+[500*x, 300*-y, 0]\n",
    "\n",
    "    raster = Raster(np.array(points[:, 0:2]), [3,3], 32633, image_data=texture)\n",
    "    metadata = { \"path\" : \"plots\" }\n",
    "    visualizer.publish_raster('{}: {}'.format(Lake_ID, image_type), metadata, raster)\n",
    "\n",
    "\n",
    "    \n",
    "#__________________________________________________________________________      \n",
    "\n",
    "#umrechnen!?:\n",
    "# gdf = geopandas.GeoDataFrame(df, geometry=gs, crs=\"EPSG:4326\")\n",
    "myProj = Proj(\"+proj=utm +zone=33 +north +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n",
    "\n",
    "print('...display the landcover plots and the era5 data  @ 3d viewer:')\n",
    "for folder in os.listdir(path):\n",
    "    if 'ID' in folder: \n",
    "        for files in os.listdir(path + '/' + folder):\n",
    "            if '.shp' in files:\n",
    "                file_input = path + '/' + folder + '/' + files\n",
    "                Lake_ID = files.split('_')[-1][0:-4]\n",
    "                \n",
    "                ds = geopandas.read_file(file_input)\n",
    "                ori = myProj(float(ds.bounds.maxx), float(ds.bounds.maxy))\n",
    "                origin = np.array([list(ori)[0],list(ori)[1],0])\n",
    "                \n",
    "                for file in os.listdir(path + '/' + folder):\n",
    "                    if '.jpg'in file and 'landcover' in file:\n",
    "                        image_path = path + '/' + folder + '/' + file\n",
    "                        image_type = 'Landcover'\n",
    "                        diplay_data(image_path, image_type)\n",
    "                    if '.jpg'in file and 'climate_data' in file:\n",
    "                        image_path = path + '/' + folder + '/' + file\n",
    "                        image_type = 'Climate Data'\n",
    "                        diplay_data(image_path, image_type)\n",
    "\n",
    "print('...done')\n",
    "\n",
    "\n",
    "#ds.bounds                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
